# Wasserstein GANs: Research Analysis & Insights

This repository presents a **mini research project on Wasserstein Generative Adversarial Networks (WGANs)**, conducted through an in-depth review of foundational and contemporary research papers combined with experimental evaluation and analytical insights.

The study focuses on understanding **training instability in traditional GANs** and how the **Wasserstein distance formulation** improves convergence, gradient behavior, and generative performance.

---

## ðŸ“Œ Research Objective

The primary goal of this project was to:
- Analyze the theoretical foundations of WGANs
- Compare WGAN variants against traditional GAN training dynamics
- Investigate stability, convergence behavior, and loss interpretability
- Propose practical insights and improvements based on empirical findings

---

## ðŸ“š Literature Review

The research involved reviewing and synthesizing findings from **multiple peer-reviewed papers**, including:
- Original Wasserstein GAN formulation
- WGAN with Gradient Penalty (WGAN-GP)
- Studies on mode collapse, Lipschitz constraints, and critic optimization

This review informed the experimental design and evaluation strategy.

---

## ðŸ§  Core Concepts Explored

- Wasserstein (Earth Moverâ€™s) Distance
- Critic vs Discriminator architectures
- Lipschitz continuity constraints
- Gradient penalty vs weight clipping
- Training stability and convergence behavior
- Loss interpretability and optimization dynamics

---

## ðŸ”¬ Experimental Evaluation

- Compared GAN and WGAN training behaviors under controlled settings
- Analyzed loss curves, convergence speed, and gradient stability
- Observed reduced mode collapse and smoother optimization trajectories in WGAN-based models
- Evaluated the impact of gradient penalty on training robustness

---
